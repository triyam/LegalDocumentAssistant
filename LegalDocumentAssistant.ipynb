{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installations"
      ],
      "metadata": {
        "id": "y0-7hxWn4cVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai faiss-cpu sentence-transformers gradio PyPDF2 spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "P1uhM_6q2G87",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the Libraries"
      ],
      "metadata": {
        "id": "41uOZKJ64j9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "import requests\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "ostGDqdt6qjg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Hugging Face API key"
      ],
      "metadata": {
        "id": "zPjK7Fyy4sFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HUGGING_FACE_API_KEY = \"huggingFaceApiKey\""
      ],
      "metadata": {
        "id": "mL03E4kg31vp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global variables\n"
      ],
      "metadata": {
        "id": "qmtzPNqU4xnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = None\n",
        "stored_documents = []"
      ],
      "metadata": {
        "id": "W43ZSke131xC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions\n"
      ],
      "metadata": {
        "id": "OkWIl06B41ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(file):\n",
        "    \"\"\"Extract text from uploaded file.\"\"\"\n",
        "    if file.name.endswith(\".pdf\"):\n",
        "        from PyPDF2 import PdfReader\n",
        "        pdf_reader = PdfReader(file)\n",
        "        return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n",
        "    elif file.name.endswith(\".txt\"):\n",
        "        return file.read().decode(\"utf-8\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def preprocess_documents(file_text, metadata=None):\n",
        "    \"\"\"Preprocess and split text into chunks.\"\"\"\n",
        "    global stored_documents\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    document = Document(page_content=file_text, metadata=metadata or {})\n",
        "    chunks = text_splitter.split_documents([document])\n",
        "    stored_documents.extend(chunks)  # Store processed documents\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def summarize_text_with_hugging_face(text, model_name=\"facebook/bart-large-cnn\"):\n",
        "    from transformers import pipeline\n",
        "    summarizer = pipeline(\"summarization\", model=model_name)\n",
        "    max_length = 200  # Set a reasonable max length for summaries\n",
        "    min_length = 100   # Ensure summaries are not too short\n",
        "    summary = summarizer(text, max_length=max_length, min_length=min_length, truncation=True)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "def create_vectorstore(documents):\n",
        "    \"\"\"Create or update a FAISS vectorstore.\"\"\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "    return FAISS.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "1Wf7dd2k37Ln"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio App Functions\n"
      ],
      "metadata": {
        "id": "pfrURLvy438e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_and_process(file, jurisdiction, date):\n",
        "    \"\"\"Handle file upload and metadata tagging.\"\"\"\n",
        "    global vectorstore\n",
        "    file_text = process_file(file)\n",
        "    metadata = {\"jurisdiction\": jurisdiction, \"date\": date}\n",
        "    documents = preprocess_documents(file_text, metadata)\n",
        "    vectorstore = create_vectorstore(documents)\n",
        "    return \"Document uploaded and processed successfully!\"\n",
        "\n",
        "\n",
        "def ask_question_with_filters(query, jurisdiction, date, chat_history=[]):\n",
        "    \"\"\"Query the document with optional filters.\"\"\"\n",
        "    global stored_documents, vectorstore\n",
        "\n",
        "    # Filter documents by jurisdiction and date\n",
        "    filtered_documents = [\n",
        "        doc for doc in stored_documents\n",
        "        if (not jurisdiction or doc.metadata.get(\"jurisdiction\") == jurisdiction) and\n",
        "           (not date or doc.metadata.get(\"date\") == date)\n",
        "    ]\n",
        "\n",
        "    if not filtered_documents:\n",
        "        chat_history.append((query, \"No matching documents found.\"))\n",
        "        return chat_history, chat_history\n",
        "\n",
        "    # Create a temporary vectorstore for filtered documents\n",
        "    temp_vectorstore = FAISS.from_documents(filtered_documents, vectorstore.embedding_function)\n",
        "    retriever = temp_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "    # Retrieve relevant documents\n",
        "    relevant_documents = retriever.get_relevant_documents(query)\n",
        "\n",
        "    # Combine the content of retrieved documents for summarization\n",
        "    combined_text = \" \".join([doc.page_content for doc in relevant_documents])\n",
        "\n",
        "    # Summarize the combined text\n",
        "    summarized_answer = summarize_text_with_hugging_face(combined_text)\n",
        "\n",
        "    # Format chatbot response\n",
        "    chat_history.append((query, summarized_answer))\n",
        "    return chat_history, chat_history"
      ],
      "metadata": {
        "id": "1xJK9qV94BT_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio UI"
      ],
      "metadata": {
        "id": "7VMCMWHP46J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"# Legal Document Assistant\")\n",
        "\n",
        "    with gr.Row():\n",
        "        file_input = gr.File(label=\"Upload Legal Document\")\n",
        "        jurisdiction_input = gr.Textbox(label=\"Jurisdiction (optional)\", placeholder=\"e.g., US, EU\")\n",
        "        date_input = gr.Textbox(label=\"Date (optional)\", placeholder=\"e.g., 2022-11-20\")\n",
        "        upload_button = gr.Button(\"Upload and Process\")\n",
        "        upload_output = gr.Textbox(label=\"Upload Status\", interactive=False)\n",
        "\n",
        "    upload_button.click(\n",
        "        upload_and_process,\n",
        "        inputs=[file_input, jurisdiction_input, date_input],\n",
        "        outputs=upload_output\n",
        "    )\n",
        "\n",
        "    chatbot = gr.Chatbot()\n",
        "    query_input = gr.Textbox(label=\"Ask a Legal Question\")\n",
        "    filter_jurisdiction = gr.Textbox(label=\"Filter by Jurisdiction (optional)\")\n",
        "    filter_date = gr.Textbox(label=\"Filter by Date (optional)\")\n",
        "    state = gr.State([])\n",
        "    ask_button = gr.Button(\"Ask\")\n",
        "\n",
        "    ask_button.click(\n",
        "        ask_question_with_filters,\n",
        "        inputs=[query_input, filter_jurisdiction, filter_date, state],\n",
        "        outputs=[chatbot, state]\n",
        "    )\n",
        "\n",
        "app.launch(debug=True)"
      ],
      "metadata": {
        "id": "t46h56EG4DfX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}